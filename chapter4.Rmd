---
title: "chapter4.Rmd"
author: "Ella"
date: "15 helmikuuta 2017"
output: html_document
---

#Chapter 4: Clustering and classification

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##2. Load the Boston data
```{r}
library(MASS)
data("Boston")
```


Now when I have accessed the Boston data, I shall present the structure and dimensions of the data frame.
```{r}
dim(Boston)
```


```{r}
str(Boston)
```

Here we can explore and find that the Boston data has 14 variables and 506 observations. The first variable 'crim' is measured by per capita crime rate by town. The 'zn' variable measures the proportion of residential land zoned for lots over 25,000 sq.ft. The 'black' variable measures the proportions of dark skinned population in a specific town. The 'lstat' is a measurement of how many people there are in the lower status population in procent. The value 'ptratio'is the ratio between the teacher and the students in a specific town. 

This 'Boston' data frame tries to gather data regarding crime rates, schools, taxes and the ratio of the black community. 

##3. Graphical overview and summaries

###Summary of the Boston data frame

```{r}
summary(Boston)
```
The variables I will be focusing on in this task is the 'crime' variable and 'black' variable. From the summary command we can find the distribution of per capita crime rate per town, and notice that most of the town don't have a major crime issue, but there is still the maximum value of 88.97. The variable 'black' shows the population of the black colored community in a town. The minimun value is very low (0.32), but it increses rapidly when we explore the first quartile with the number being 375,38. After this the values don't increase as much, because the maximum value is 396,90.


###Graphical overview of the Boston data

```{r}
pairs(Boston)
```

From this table it is quite difficult to explore the relationships between the different variables at first glance. If we look closley we can see different causal relationships between the variables. In these tasks we will be looking more closley on the 'crim' (crime) varianble, and how these other variables contributes to the crime rates. 

So I can try to visualize the distributions with a correlation plot as in the Datacamp exercises before.

```{r}
library(tidyverse)
```
```{r}
library(corrplot)
```

```{r}
cor_matrix<-cor(Boston) %>% round(digits = 2)
```


```{r}
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

From this correlation plot we can easily spot which variables has some kind of causal relationship with one another. The dark red color indicates a strong negative correlation and the dark blue color indicates a strong positive correlation, and obviously the lighter colors indicate a less strong positive or negative correlation between the variables. 


##4. Standardizing the data set

Before we can easily compare the different variables, we shall scale the values, so they are compatable with one another. 
```{r}
boston_scaled <- scale(Boston)
```

```{r}
summary(boston_scaled)
```
```{r}
class(boston_scaled)
```

```{r}
boston_scaled <-as.data.frame(boston_scaled)
```

From this new scaled variable we can see that all the variable values range between -10 and 10. This will probably lead to an easier way to compare the different variables.


Now I shall create a chategorical variable of the crime rates in Boston. I will do this, because it is not possible to use the numerical values in some of the codes that we have to make so we get the wanted results. 


```{r}
scaled_crim <-boston_scaled$crim
```

Now I need to use the quantiles as break points in the variable. The quantiles are 0%, 25%, 50%, 75% and 100%. This makes it easier to understand how the values are distributed in a specific variable. The variable that I am using is the 'crim' variable. 

```{r}
bins <- quantile(scaled_crim)
```

```{r}
bins
```
As we can see, I have created breaking points at the quantlies. So from these we can interpet that half of the variable values are between values -0,419 and -0,390. So we can conclude that there isn't a very big difference between the first 50% of the data. The big difference is found between the last 25% (75%-100%), where the values are distributed between 0,00738 and 9,9241. I would interpet this as there being a few towns with extremely high crime rates in comparison to other towns. 

Next I will create a new variable 'crime' that consists of the scaled crim values, its breaks consists of the quartiles, and we are labeling 4 different groupings on the rates of crimes. The groupings are low-, medium low-, medium high- and very high crime rates. 

```{r}
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

```

```{r}
table(crime)
```

Next I will have to drop the old crime rate ('crim') variable from the data set. This I will do, because the new crime rate is more sufficient to my needs of analysis. 

```{r}
boston_scaled <- dplyr::select(boston_scaled, -crim)
```

```{r}
boston_scaled <- data.frame(boston_scaled, crime)
```


Next I shall divide the scaled_boston data into two different data sets; 'test' and 'train'. But before I can divide the data to two new data sets, I need to figure out how many rows the boston_scaled variable has. The number of rows is saved as he variable 'n'. 
```{r}
n <-nrow(boston_scaled)
```

Next I need to take a sample of the data, and in the task 80% ov the data will go to the 'train' data set, so we need a sample that cover 80% of the scaled_boston data set. This variable shall be named ind.

```{r}
ind <- sample(n,  size = n * 0.8)
```

Now we can divide the scaled_data set into the two new data sets 'train' and 'test'

```{r}
train <- boston_scaled[ind,]
```

```{r}
test <- boston_scaled[-ind,]
```




```{r}
correct_classes <- test$crime
```

```{r}
test <- dplyr::select(test, -crime)
```

```{r}
library("MASS")
```



##5. Linear discriminant analysis: train set

In the linear discriminant analysis I will be looking at the target variable crime, and all the other values are predictors of the crime value. We can also see the predictor variables being grouped by the labels low, med_low, med_high and high. At the bottom we can also find the coefficients of the linear discriminants. 


```{r}
lda.fit <- lda(crime ~ ., data = train)
```


```{r}
lda.fit
```


Next we want to visualize the distribution of these different variables are distributed. The different labeling chategorize are coded in different colors. The low value is black, med_low is red, med_high is green and the high values are blue. The different variables are also coded in ted, and they also have arrows that point in what direction they move. 

```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
```

```{r}
classes <- as.numeric(train$crime)
```



```{r}
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

##6. Predicting the classes

Next task is to predict classes. This I will do with the test data frame. The prediction is made to evaluate what kind of values a specific variable is supposed to give, based upon some values. 

```{r}
lda.pred <- predict(lda.fit, newdata = test)
```

```{r}
table(correct = correct_classes, predicted = lda.pred$class)
```

Here we can compare between the actual values and the predicted values and how much they differ from each other. 

From this table we can see that when the correct low value is 13, the prediction gives out 10 on med_low. Obviously this is an erroe, but because of my inexperience, I have a hard time judgeing if this is a minor or a major flaw. 

##7. Reloading the Boston data set






